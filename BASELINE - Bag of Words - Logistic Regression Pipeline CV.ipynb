{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Baseline Model (TF-IDF into Logistic Regression)\n",
    "\n",
    "Generated features consists of word-based and character-based TF-IDF (term frequency - inverse document frequency) values.\n",
    "\n",
    "The baseline model is a simple logistic regression.\n",
    "\n",
    "Random Search CV hunts for the optimal vectorization and model parameters, achieving a local CV ROC-AUC score of 98.48. Pretty good for such a simple model! The substantially more complex neural models that will be developed later will achieve ROC-AUC scores of 99.5, but at the cost of much longer training times and computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('single_model_predictions/train_split.csv').fillna(' ')\n",
    "valid = pd.read_csv('single_model_predictions/valid/valid_split.csv').fillna(' ')\n",
    "test  = pd.read_csv('test.csv').fillna(' ')\n",
    "\n",
    "train_text = train['comment_text']\n",
    "valid_text = valid['comment_text']\n",
    "test_text  = test['comment_text']\n",
    "\n",
    "all_text = pd.concat([train_text, valid_text, test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Words and Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  analyzer='word',\n",
    "                                  token_pattern=r'\\w{1,}',\n",
    "                                  max_features=10000)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                  strip_accents='unicode',\n",
    "                                  analyzer='char',\n",
    "                                  max_features=10000)\n",
    "\n",
    "classifier = LogisticRegression(solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_char_vectorizer = FeatureUnion([\n",
    "    ('word_vect', word_vectorizer),\n",
    "    ('char_vect', char_vectorizer),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', word_char_vectorizer),\n",
    "    ('clf', classifier),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"vect__word_vect__ngram_range\":  [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)],\n",
    "    \"vect__char_vect__ngram_range\":  [(1, 1), (1, 3), (1, 5), (2, 3), (2, 5), (3, 5)],\n",
    "    \"clf__C\":                        [0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search CV over the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Done  36 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=7)]: Done  60 out of  60 | elapsed: 52.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 3234.59 seconds for 20 candidates parameter settings.\n",
      "\n",
      "Best Score = 0.977276937629\n",
      "\n",
      "Best Parameters = {'vect__word_vect__ngram_range': (1, 1), 'vect__char_vect__ngram_range': (1, 3), 'clf__C': 1}\n"
     ]
    }
   ],
   "source": [
    "n_iter_search = 20;\n",
    "\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=n_iter_search, cv=3, \n",
    "                                   scoring='roc_auc', n_jobs=7, verbose=1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "random_search.fit(train['comment_text'].values, train[class_names[0]].values)\n",
    "\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "print(\"\\nBest Score = \" + str(random_search.best_score_))\n",
    "\n",
    "print(\"\\nBest Parameters = \" + str(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions  = {'id': test['id']}\n",
    "test_predictions[class_names[0]] = random_search.predict_proba(test['comment_text'].values)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9772769088526844\n",
      "CV score for class severe_toxic is 0.9885642581995572\n",
      "CV score for class obscene is 0.9898989711408408\n",
      "CV score for class threat is 0.9884735470340512\n",
      "CV score for class insult is 0.9817733068152982\n",
      "CV score for class identity_hate is 0.9828762819399839\n",
      "Total CV score is 0.9848105456637359\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "test_predictions  = {'id': test['id']}\n",
    "valid_predictions = {'id': valid['id']}\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = pipeline.set_params(clf__C=1, vect__word_vect__ngram_range=(1,1), vect__char_vect__ngram_range=(1, 3))\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train['comment_text'].values, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train['comment_text'].values, train_target)\n",
    "    test_predictions[class_name] = classifier.predict_proba(test['comment_text'].values)[:, 1]\n",
    "    valid_predictions[class_name] = classifier.predict_proba(valid['comment_text'].values)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9703823806373831\n",
      "CV score for class severe_toxic is 0.9850204552076671\n",
      "CV score for class obscene is 0.9840357267528151\n",
      "CV score for class threat is 0.9854014125653093\n",
      "CV score for class insult is 0.9774041293366643\n",
      "CV score for class identity_hate is 0.9741863733994759\n",
      "Total CV score is 0.979405079649886\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "test_predictions  = {'id': test['id']}\n",
    "valid_predictions = {'id': valid['id']}\n",
    "\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    test_predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "    valid_predictions[class_name] = classifier.predict_proba(valid_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_submission = pd.DataFrame.from_dict(test_predictions)\n",
    "test_submission.to_csv('single_model_predictions/other/test/18_02_16_BagOfWords_TFIDF_LogisticRegression_Test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_submission = pd.DataFrame.from_dict(valid_predictions)\n",
    "valid_submission.to_csv('single_model_predictions/other/valid/18_02_16_BagOfWords_TFIDF_LogisticRegression_Valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
